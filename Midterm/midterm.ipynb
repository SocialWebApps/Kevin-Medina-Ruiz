{
 "metadata": {
  "name": "",
  "signature": "sha256:6603c89483e60c5451c4f696c360649978b7dfe70004ba3b651fdf9881302849"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Kevin Medina"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Midterm : Social Web Apps"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import twitter\n",
      "\n",
      "# XXX: Go to http://dev.twitter.com/apps/new to create an app and get values\n",
      "# for these credentials, which you'll need to provide in place of these\n",
      "# empty string values that are defined as placeholders.\n",
      "# See https://dev.twitter.com/docs/auth/oauth for more information \n",
      "# on Twitter's OAuth implementation.\n",
      "\n",
      "CONSUMER_KEY = ''  \n",
      "CONSUMER_SECRET = ''\n",
      "OAUTH_TOKEN = ''\n",
      "OAUTH_TOKEN_SECRET = ''\n",
      "\n",
      "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
      "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
      "\n",
      "twitter_api = twitter.Twitter(auth=auth)\n",
      "\n",
      "# Nothing to see by displaying twitter_api except that it's now a\n",
      "# defined variable\n",
      "\n",
      "print twitter_api"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<twitter.api.Twitter object at 0x0000000003DCD5C0>\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Purpose"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The purpose of this exercise is to extract shortened bit.ly urls from the top 5 files from  http://1usagov.measuredvoice.com/2013/ . Then search the tweets in a day for the mentioned set of the shortened urls. \n",
      "Once this is achieved the top 10 mentioned urls will be extracted from the twitter search. \n",
      "The tweets mentioning these urls will then be plotted using the static google map API. The 'geo' field will be used to extract out the coordinates of the tweets. \n",
      "\n",
      "***Note: many tweets have 'null' for geo data, therefore, the actual number of tweets that can be plotted is limited***\n",
      "\n",
      "The government data files are in the \"resources\" folder provided.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Step #1: Organizing the gov data files into a single record file"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reference: The following code belongs to Debra Lewis from the following project : https://github.com/SocialWebApps/Debra-Lewis/tree/master/Assignment-2 ."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Provide file names for the files in the resources folder\n",
      "path = \"resources\"\n",
      "\n",
      "filename1 = \"data1.txt\"\n",
      "pathname1 = path + \"\\\\\" + filename1\n",
      "\n",
      "filename2 = \"data2.txt\"\n",
      "pathname2 = path + \"\\\\\" + filename2\n",
      "\n",
      "filename3 = \"data3.txt\"\n",
      "pathname3 = path + \"\\\\\" + filename3\n",
      "\n",
      "filename4 = \"data4.txt\"\n",
      "pathname4 = path + \"\\\\\" + filename4\n",
      "\n",
      "filename5 = \"data5.txt\"\n",
      "pathname5 = path + \"\\\\\" + filename5\n",
      "\n",
      "#open and append each record into one single file\n",
      "file1 = open(pathname1,'r')    \n",
      "file1.readline()\n",
      "records = [json.loads(line) for line in file1]\n",
      "file1.close()\n",
      "print \"Number of records after reading from file 1: \" + str(len(records))\n",
      "\n",
      "file2 = open(pathname2,'r')   \n",
      "file2.readline()\n",
      "for line in file2:\n",
      "    records.append(json.loads(line))\n",
      "file2.close()\n",
      "print \"Number of records after reading from file 2: \" + str(len(records))\n",
      "\n",
      "file3 = open(pathname3,'r')    \n",
      "file3.readline()\n",
      "for line in file3:\n",
      "    records.append(json.loads(line))\n",
      "file3.close()\n",
      "print \"Number of records after reading from file 3: \" + str(len(records))\n",
      "\n",
      "file4 = open(pathname4,'r')    \n",
      "file4.readline()\n",
      "for line in file4:\n",
      "    records.append(json.loads(line))\n",
      "file4.close()\n",
      "print \"Number of records after reading from file 4: \" + str(len(records))\n",
      "\n",
      "file5 = open(pathname5,'r')    \n",
      "file5.readline()\n",
      "for line in file5:\n",
      "    records.append(json.loads(line))\n",
      "file5.close()\n",
      "print \"Number of records after reading from file 5: \" + str(len(records))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of records after reading from file 1: 9223\n",
        "Number of records after reading from file 2: 16913"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of records after reading from file 3: 21852"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of records after reading from file 4: 26495"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Number of records after reading from file 5: 30453"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "If done correctly the output should be the following :\n",
      "    \n",
      "Number of records after reading from file 1: 9223\n",
      "Number of records after reading from file 2: 16913\n",
      "Number of records after reading from file 3: 21852\n",
      "Number of records after reading from file 4: 26495\n",
      "Number of records after reading from file 5: 30453"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Step #2: Creating a set of possible bit.ly shortened urls"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In this step we will extract shortened urls from the gov data records. The json data dictionary is the following:\n",
      "\n",
      "{\n",
      "        \"a\": USER_AGENT, \n",
      "        \"c\": COUNTRY_CODE, # 2-character iso code\n",
      "        \"nk\": KNOWN_USER,  # 1 or 0. 0=this is the first time we've seen this browser\n",
      "        \"g\": GLOBAL_BITLY_HASH, \n",
      "        \"h\": ENCODING_USER_BITLY_HASH,\n",
      "        \"l\": ENCODING_USER_LOGIN,\n",
      "        \"hh\": SHORT_URL_CNAME,\n",
      "        \"r\": REFERRING_URL,\n",
      "        \"u\": LONG_URL,\n",
      "        \"t\": TIMESTAMP,\n",
      "        \"gr\": GEO_REGION,\n",
      "        \"ll\": [LATITUDE, LONGITUDE],\n",
      "        \"cy\": GEO_CITY_NAME,\n",
      "        \"tz\": TIMEZONE # in http://en.wikipedia.org/wiki/Zoneinfo format\n",
      "        \"hc\": TIMESTAMP OF TIME HASH WAS CREATED, \n",
      "        \"al\": ACCEPT_LANGUAGE http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4 \n",
      " }"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is found at http://www.usa.gov/About/developer-resources/1usagov.shtml#About_The_Data \n",
      "\n",
      "***We will ensure that the 'hh' field is \"bit.ly\" and use the url in the 'r' field to create our list of shortened urls.\n",
      "In addition, sometimes the  'r' field returns \"direct\". These are not valid shortened urls so they will be ignored.\n",
      "Furthermore, all URLs that do not begin with \"http://t.co\" are not .gov websites and therefore also not valid and will be ignored as well***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Create a url array to be able to count the frequency of each url\n",
      "urls = []\n",
      "\n",
      "for rec in range(len(records)):\n",
      "    if(records[rec].has_key('hh')):\n",
      "        if(records[rec]['hh'] == \"bit.ly\" or records[rec]['hh'] == \"1.usa.gov\"):    #ensure the record has an 'r' field & == \"bit.ly\" | \"1.usa.gov\"\n",
      "            if(records[rec]['r'] !=  \"direct\" and records[rec]['r'].startswith('http://t.co')):     #ensure there is a valid shortened url\n",
      "                urls.append(records[rec]['r'])                              #add url to urls array\n",
      "            \n",
      "#Create a list of the set of urls to eliminate multiple occurances\n",
      "urls_set_list = list(set(urls))\n",
      "\n",
      "#print the difference in all urls and the set of the urls\n",
      "print \"Number of entries that had an 'hh' field equal to 'bit.ly' whose 'r' field was not equal to 'direct' and began with 'http://t.co': \",len(urls)\n",
      "print \"Number of unique urls found(set of urls):\" , len(urls_set_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of entries that had an 'hh' field equal to 'bit.ly' whose 'r' field was not equal to 'direct' and began with 'http://t.co':  4857\n",
        "Number of unique urls found(set of urls): 1110\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The expected output should be two numbers. \n",
      "\n",
      "The first number is the number of entries that had an 'hh' field equal to \"bit.ly\" whose 'r' field was not equal to \"direct\".\n",
      "\n",
      "The second number is the total number of unique urls found.\n",
      "\n",
      "My ouput was the following:\n",
      "\n",
      "Number of entries that had an 'hh' field equal to 'bit.ly' whose 'r' field was not equal to 'direct' and began with 'http://t.co': 4857\n",
      "Number of unique urls found(set of urls): 1110\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Step #3: Displaying 10 most common shortened URLs in gov data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this step, we will simply display the top 10 shortened URLs found in the 5 gov data files.\n",
      "This was not required in the assignment but it will provide a good point to compare to the top 10 mentioned URLs in twitter for the specified time range."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "for item in [urls]:\n",
      "    c = Counter(item)\n",
      "    print c.most_common()[:10] # top 10\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'http://t.co/Dv6Jqbwu8H', 520), (u'http://t.co/Hq3x89STor', 286), (u'http://t.co/psBn8njvIB', 153), (u'http://t.co/rVNsSXG93m', 153), (u'http://t.co/uL84T6STJV', 123), (u'http://t.co/yZ5fVLs44B', 99), (u'http://t.co/s307mx2qGk', 96), (u'http://t.co/AFTsMdeDxj', 79), (u'http://t.co/0BgxXOVREz', 71), (u'http://t.co/4wI4OK1Xdk', 67)]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from prettytable import PrettyTable\n",
      "    \n",
      "x = PrettyTable([\"URL\", \"Count\"])\n",
      "x.padding_width = 1\n",
      "\n",
      "#find the frequency of each url\n",
      "c = Counter(urls)\n",
      "\n",
      "#turn into a list for easy indexing\n",
      "c_list=list(c.most_common())\n",
      "\n",
      "#display top 10 urls and their counts\n",
      "[ x.add_row(kv) for kv in c.most_common()[:10] ]\n",
      "x.align['URL'], x.align['Count'] = 'l', 'r' # Set column alignment\n",
      "\n",
      "print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+------------------------+-------+\n",
        "| URL                    | Count |\n",
        "+------------------------+-------+\n",
        "| http://t.co/Dv6Jqbwu8H |   520 |\n",
        "| http://t.co/Hq3x89STor |   286 |\n",
        "| http://t.co/psBn8njvIB |   153 |\n",
        "| http://t.co/rVNsSXG93m |   153 |\n",
        "| http://t.co/uL84T6STJV |   123 |\n",
        "| http://t.co/yZ5fVLs44B |    99 |\n",
        "| http://t.co/s307mx2qGk |    96 |\n",
        "| http://t.co/AFTsMdeDxj |    79 |\n",
        "| http://t.co/0BgxXOVREz |    71 |\n",
        "| http://t.co/4wI4OK1Xdk |    67 |\n",
        "+------------------------+-------+\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The ouput should be a pretty printed table like the one below:\n",
      "\n",
      "+------------------------+-------+\n",
      "| URL                    | Count |\n",
      "+------------------------+-------+\n",
      "| http://t.co/Dv6Jqbwu8H |   520 |\n",
      "| http://t.co/Hq3x89STor |   286 |\n",
      "| http://t.co/psBn8njvIB |   153 |\n",
      "| http://t.co/rVNsSXG93m |   153 |\n",
      "| http://t.co/uL84T6STJV |   123 |\n",
      "| http://t.co/yZ5fVLs44B |    99 |\n",
      "| http://t.co/s307mx2qGk |    96 |\n",
      "| http://t.co/AFTsMdeDxj |    79 |\n",
      "| http://t.co/0BgxXOVREz |    71 |\n",
      "| http://t.co/4wI4OK1Xdk |    67 |\n",
      "+------------------------+-------+"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Step #4: Resolving \"http://t.co\" into \"1.usa.gov\""
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In order to ensure my process was correct I tweeted out the top 3 shortened URLs found in step #3. \n",
      "What I found was that if you attempt to tweet out say \"http://t.co/Dv6Jqbwu8H\" twitter actually resolves it to \"http://1.usa.gov/16Ewvc4\".\n",
      "Both links lead to the same page. \n",
      "\n",
      "The cell below shows the functionality of the twitter-text-python library that helps determine what twitter resolves these urls to."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import twitter-text-python\n",
      "from ttp import ttp\n",
      "from ttp import utils\n",
      "\n",
      "#Finding what twitter resolves the shortened url to\n",
      "p = ttp.Parser()\n",
      "result = p.parse(\"http://t.co/Dv6Jqbwu8H\")\n",
      "dict = utils.follow_shortlinks(result.urls)\n",
      "print dict['http://t.co/Dv6Jqbwu8H'][1]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://1.usa.gov/16Ewvc4\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output should be http://1.usa.gov/16Ewvc4"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Step #5: Search for each URL mention in twitter & assign frequency"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this step we search twitter for the the mention of the URLs. The assignment calls for a restriction of a time frame either a day or a week. Therefore, I have selected my time frame to be 1 week. Since the search APi only allows search one week old I do not have to specify the 'since' or 'until' fields in my query.\n",
      "\n",
      "\n",
      "We will use the process described in step # 4 to get the correct 'q' parameter to search twitter for.\n",
      "\n",
      "In my case I limited my search to the top 50 URLs found in step # 3. This is because we have a set of '1110'(as found in step # 2) and the rate limit for twitter is 180 queries per 15 minutes, In addition, some URLSs might not be active anymore so they can throw timeout errors when twitter is trying to resolve them.. To prevent a violation of the rate limit I limited my search to just 50 queries. You can change this by chnaging the range in the first for loop as you desire.\n",
      "\n",
      "***NOTE: You can only retrieve tweets a week old using the search twitter API. ***\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import unquote to prevent url encoding errors in next_results\n",
      "from urllib import unquote\n",
      "\n",
      "url_frequency_list = []\n",
      "dict = {}\n",
      "\n",
      "#Iterate through the urls and determine their frequency\n",
      "for x in range (0,50):\n",
      "    #Use the process described in step #4 to get correct q parameter\n",
      "    t_co_url = p.parse(c_list[x][0])\n",
      "    dict = utils.follow_shortlinks(t_co_url.urls)\n",
      "    q = dict[c_list[x][0]][1]\n",
      "    print q\n",
      "   \n",
      "    count = 100\n",
      "\n",
      "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets\n",
      "\n",
      "    search_results = twitter_api.search.tweets(q=q,count=count,include_entities=True)\n",
      "\n",
      "    statuses = search_results['statuses']\n",
      "\n",
      "\n",
      "    # Iterate through 5 more batches of results by following the cursor\n",
      "\n",
      "    for _ in range(5):\n",
      "        #print \"Length of statuses\", len(statuses)\n",
      "        try:\n",
      "            next_results = search_results['search_metadata']['next_results']\n",
      "        except KeyError, e: # No more results when next_results doesn't exist\n",
      "            break\n",
      "        \n",
      "        # Create a dictionary from next_results, which has the following form:\n",
      "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
      "        kwargs = dict([kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])    \n",
      "\n",
      "    \n",
      "        search_results = twitter_api.search.tweets(**kwargs)\n",
      "        statuses += search_results['statuses']\n",
      "    \n",
      "    \n",
      "    url_freq_tuple = (q,len(statuses),statuses)\n",
      "    url_frequency_list.append(url_freq_tuple)\n",
      "\n",
      "#We will sort on frequency which is item number 1 in tuple\n",
      "def getKey(item):\n",
      "    return item[1]\n",
      "\n",
      "#sort the urls\n",
      "sorted_urls = sorted(url_frequency_list,key=getKey,reverse=True)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The output is just the 'q' parameters used for each query. Since I did 50 queries the output is 50 URLs. \n",
      "\n",
      "A snippet of my output was the following:\n",
      "\n",
      "http://1.usa.gov/16Ewvc4\n",
      "http://1.usa.gov/107xZnW\n",
      "http://1.usa.gov/10ydrrU\n",
      "http://1.usa.gov/14wjbCv\n",
      "http://1.usa.gov/10AXKvO\n",
      "http://1.usa.gov/11KTaTe\n",
      "http://1.usa.gov/18NUoNR\n",
      "http://1.usa.gov/18QIXVE\n",
      "http://1.usa.gov/109ar5E\n",
      "http://1.usa.gov/14cUTOu\n",
      "http://go.nasa.gov/12AyUk1\n",
      "http://1.usa.gov/12AyCcX\n",
      "http://1.usa.gov/16EzBNe\n",
      "http://1.usa.gov/16rF9vf\n",
      "http://1.usa.gov/ZInfMm\n",
      "http://1.usa.gov/10DiYcF"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Step #6: Display top 10 URLs mentioned in twitter during the specified time frame"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this step we simply display the top 10 most frequent URLs found in step # 5."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = PrettyTable([\"URL\", \"Count\"])\n",
      "x.padding_width = 1\n",
      "\n",
      "for y in range(0,10):\n",
      "    x.add_row([sorted_urls[y][0],sorted_urls[y][1]])\n",
      "\n",
      "x.align['URL'], x.align['Count'] = 'l', 'r' # Set column alignment\n",
      "\n",
      "print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+--------------------------+-------+\n",
        "| URL                      | Count |\n",
        "+--------------------------+-------+\n",
        "| http://1.usa.gov/16Ewvc4 |     3 |\n",
        "| http://1.usa.gov/107xZnW |     2 |\n",
        "| http://1.usa.gov/10ydrrU |     2 |\n",
        "| http://1.usa.gov/107xZnW |     2 |\n",
        "| http://1.usa.gov/14wjbCv |     0 |\n",
        "| http://1.usa.gov/10AXKvO |     0 |\n",
        "| http://1.usa.gov/11KTaTe |     0 |\n",
        "| http://1.usa.gov/18NUoNR |     0 |\n",
        "| http://1.usa.gov/18QIXVE |     0 |\n",
        "| http://1.usa.gov/109ar5E |     0 |\n",
        "+--------------------------+-------+\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "***The results I got in this stage were all my tweets. I tweeted these URLs on 03/15/2015 to verify my process. Out of those tweets I only enabled the location one of them. The ouput verifies my process as it returned what I expected.***"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "The expected output is the top 10 URLs mentioned in twitter and their respective mention count.\n",
      "\n",
      "My output was the following:\n",
      "+--------------------------+-------+\n",
      "| URL                      | Count |\n",
      "+--------------------------+-------+\n",
      "| http://1.usa.gov/16Ewvc4 |     3 |\n",
      "| http://1.usa.gov/107xZnW |     2 |\n",
      "| http://1.usa.gov/10ydrrU |     2 |\n",
      "| http://1.usa.gov/107xZnW |     2 |\n",
      "| http://1.usa.gov/14wjbCv |     0 |\n",
      "| http://1.usa.gov/10AXKvO |     0 |\n",
      "| http://1.usa.gov/11KTaTe |     0 |\n",
      "| http://1.usa.gov/18NUoNR |     0 |\n",
      "| http://1.usa.gov/18QIXVE |     0 |\n",
      "| http://1.usa.gov/109ar5E |     0 |\n",
      "+--------------------------+-------+"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Step #7: Extract the coordinates from the 'coordinates' field of the 10 most mentioned URLs"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this step the coordinates from the 'geo' field in each tweet for the top 10 URLs is extracted.\n",
      "\n",
      "***NOTE: The 'geo' field is deprecated thus the coordinate field should be used.***"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Coordinates\n",
      "\n",
      "for x in range(0,10):\n",
      "    coordinate_info.append([status['coordinates'] for status in sorted_urls[x][2]])\n",
      "\n",
      "print \" ------COORDINATE INFORMATION----\"\n",
      "print json.dumps(coordinate_info[0:], indent=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "My ouput only showed one pair of coordinates.\n",
      "\n",
      "A snippet of my output was the following:\n",
      "    \n",
      "------COORDINATE INFORMATION----\n",
      "[\n",
      " [\n",
      "  null, \n",
      "  null, \n",
      "  {\n",
      "   \"type\": \"Point\", \n",
      "   \"coordinates\": [\n",
      "    -87.8972597, \n",
      "    43.0499106\n",
      "   ]\n",
      "  }\n",
      " ], \n",
      "\n",
      "As you can see this shows that out of the three mentions of the first URL only one had the coordinates. This is what was expected since I only turned on location for one tweet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Extracting coordinates from dictionary in object list for '#HappyMatturday' \n",
      "lon = []\n",
      "lat = []\n",
      "for x in coordinate_info:\n",
      "    if x != None:\n",
      "        for y in range(len(x)):\n",
      "            if x[y] != None:                \n",
      "                lon.append(x[y]['coordinates'][0])\n",
      "                lat.append(x[y]['coordinates'][1])\n",
      "\n",
      "print \"---COORDINATES FOR URL MENTIONS FOUND---\"\n",
      "print\n",
      "for x in range (len(lat)):\n",
      "    print lon[x], \",\" ,lat[x]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "---COORDINATES FOR URL MENTIONS FOUND---\n",
        "\n",
        "-87.8972597 , 43.0499106\n",
        "-87.8972597 , 43.0499106\n",
        "-87.8972597 , 43.0499106\n",
        "-87.8972597 , 43.0499106\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Output should be the coordinates (if any) extracted.\n",
      "\n",
      "My output was the following:\n",
      " \n",
      "---COORDINATES FOR URL MENTIONS FOUND---\n",
      "\n",
      "-87.8972597 , 43.0499106\n",
      "-87.8972597 , 43.0499106\n",
      "-87.8972597 , 43.0499106\n",
      "-87.8972597 , 43.0499106      "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Step #8: Plot coordinates on google map"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since our search returned mostly null for coordinates I did not have a lot of points to plot. In addition, in order to plot multiple markers on a google map you must use the static map API. The embbeded map API only allows for one marker at a time ( I have included, but commented out how to do this for reference ). The actual google MAPS API can be used to plot multiple markers but it requires javascript and it will no longer be supported soon."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import IFrame\n",
      "from IPython.core.display import display\n",
      "from IPython.core.display import Image\n",
      "\n",
      "#format our url\n",
      "\n",
      "#base url\n",
      "formattedurl = \"https://maps.googleapis.com/maps/api/staticmap?\"\n",
      "\n",
      "#add the center of our map (lat,lon), and size required if markers are not present\n",
      "formattedurl += \"size=1000x1000\"\n",
      "\n",
      "#add markers\n",
      "formattedurl += \"&markers=color:red\"\n",
      "\n",
      "for x in range (len(lat)):\n",
      "    formattedurl += \"%7C{},{}\".format(lat[x], lon[x])\n",
      "    \n",
      "print formattedurl\n",
      "\n",
      "#format url for our static map\n",
      "google_static_map = formattedurl\n",
      "\n",
      "#display our map\n",
      "Image(url=google_static_map)\n",
      "\n",
      "#EMBEDDED GOOGLE MAP EXAMPLE FOR REFERENCE\n",
      "#google_maps_url = \"https://www.google.com/maps/embed/v1/place?key=AIzaSyBpFjRP377IU68WWXHLF8GIVaE8n-ru30U&zoom=2&q=FAU\"\n",
      "\n",
      "#print \"---EMBEDDED MAP EXAMPLE---\"\n",
      "#display(IFrame(google_maps_url, '1075px', '500px'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "https://maps.googleapis.com/maps/api/staticmap?size=1000x1000&markers=color:red%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597\n"
       ]
      },
      {
       "html": [
        "<img src=\"https://maps.googleapis.com/maps/api/staticmap?size=1000x1000&markers=color:red%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597\"/>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "<IPython.core.display.Image at 0xb4762e8>"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Expected Output:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My output was correct as it plotted my current location which is in Milwaukee, WI at the moment. The link to my map is the following:\n",
      "https://maps.googleapis.com/maps/api/staticmap?size=1000x1000&markers=color:red%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597%7C43.0499106,-87.8972597    "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Summary & Conclusion:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The exercise was a success. Unfortunately, the URLs found in the gov data files are old and therefore, have not been tweeted out much. To get around this I tweeted out (@medinaruizk) tweets containing a couple of the URLs. With this I was able to confirm my process was correct as the results returned my tweets and my coordinates."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
